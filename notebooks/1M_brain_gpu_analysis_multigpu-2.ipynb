{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPIDS & Scanpy Single-Cell RNA-seq Multi-GPU Workflow on 1 Million Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020, NVIDIA CORPORATION.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\") you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0 \n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a single-cell RNA analysis workflow that begins with preprocessing a count matrix of size `(n_gene, n_cell)` and results in a visualization of the clustered cells for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we use a dataset of 1M brain cells with Unified Virtual Memory to oversubscribe GPU memory. We then use dask to scale PCA, K-means clustering, and UMAP across multiple GPUs.\n",
    "\n",
    "See the README for instructions to download this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import cupy as cp\n",
    "import cupyx as cpx\n",
    "import cupyx\n",
    "\n",
    "import cudf\n",
    "import math\n",
    "\n",
    "\n",
    "import h5py\n",
    "import scipy\n",
    "\n",
    "import dask\n",
    "import dask_cudf\n",
    "import rmm\n",
    "\n",
    "from numba import cuda\n",
    "from statsmodels import robust\n",
    "\n",
    "from dask_cuda import initialize, LocalCUDACluster\n",
    "from dask import delayed, dataframe\n",
    "from dask.dataframe.utils import make_meta\n",
    "from dask.distributed import Client, default_client\n",
    "from dask_cuda.local_cuda_cluster import cuda_visible_devices\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from cuml.common.memory_utils import with_cupy_rmm\n",
    "from cuml.common import rmm_cupy_ary\n",
    "from cuml.manifold import TSNE, UMAP\n",
    "from cuml.linear_model import LinearRegression\n",
    "\n",
    "from cuml.dask.decomposition import PCA as cu_dask_PCA\n",
    "from cuml.dask.cluster import KMeans as cu_dask_KMeans\n",
    "from cuml.dask.manifold import UMAP as cu_dask_UMAP\n",
    "from cuml.dask.linear_model import LinearRegression as cu_dask_LinearRegression\n",
    "from cuml.dask.neighbors import NearestNeighbors as cu_dask_NearestNeighbors\n",
    "\n",
    "from cuml.dask.common.dask_arr_utils import to_dask_cudf, to_sparse_dask_array\n",
    "from cuml.dask.common.dask_df_utils import to_dask_cudf\n",
    "from cuml.dask.common.input_utils import to_dask_cupy\n",
    "from cuml.dask.common.part_utils import _extract_partitions\n",
    "\n",
    "from bokeh.io.export import export_png\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models.tickers import FixedTicker\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "\n",
    "import rapids_scanpy_funcs\n",
    "import utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'Expected ')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "COLORS = [\"#000000\", \"#FFFF00\", \"#1CE6FF\", \"#FF34FF\", \"#FF4A46\", \"#008941\",\n",
    "          \"#006FA6\", \"#A30059\", \"#FFDBE5\", \"#7A4900\", \"#0000A6\", \"#63FFAC\",\n",
    "          \"#B79762\", \"#004D43\", \"#8FB0FF\", \"#997D87\", \"#5A0007\", \"#809693\",\n",
    "          \"#FEFFE6\", \"#1B4400\", \"#4FC601\", \"#3B5DFF\", \"#4A3B53\", \"#FF2F80\",\n",
    "          \"#61615A\", \"#BA0900\", \"#6B7900\", \"#00C2A0\", \"#FFAA92\", \"#FF90C9\", \n",
    "          \"#B903AA\", \"#D16100\", \"#DDEFFF\", \"#000035\", \"#7B4F4B\", \"#A1C299\",\n",
    "          \"#300018\", \"#0AA6D8\", \"#013349\", \"#00846F\", \"#372101\", \"#FFB500\",\n",
    "          \"#C2FFED\", \"#A079BF\", \"#CC0744\", \"#C0B9B2\", \"#C2FF99\", \"#001E09\",\n",
    "          \"#00489C\", \"#6F0062\", \"#0CBD66\", \"#EEC3FF\", \"#456D75\", \"#B77B68\",\n",
    "          \"#7A87A1\", \"#788D66\", \"#885578\", \"#FAD09F\", \"#FF8A9A\", \"#D157A0\",\n",
    "          \"#BEC459\", \"#456648\", \"#0086ED\", \"#886F4C\", \"#34362D\", \"#B4A8BD\",\n",
    "          \"#00A6AA\", \"#452C2C\", \"#636375\", \"#A3C8C9\", \"#FF913F\", \"#938A81\",\n",
    "          \"#575329\", \"#00FECF\", \"#B05B6F\", \"#8CD0FF\", \"#3B9700\", \"#04F757\",\n",
    "          \"#C8A1A1\", \"#1E6E00\", \"#7900D7\", \"#A77500\", \"#6367A9\", \"#A05837\",\n",
    "          \"#6B002C\", \"#772600\", \"#D790FF\", \"#9B9700\", \"#549E79\", \"#FFF69F\",\n",
    "          \"#201625\", \"#72418F\", \"#BC23FF\", \"#99ADC0\", \"#3A2465\", \"#922329\",\n",
    "          \"#5B4534\", \"#FDE8DC\", \"#404E55\", \"#0089A3\", \"#CB7E98\", \"#A4E804\",\n",
    "          \"#324E72\", \"#6A3A4C\",]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the RAPIDS memory manager to enable Unified Virtual Memory management, which allows us to oversubscribe the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>ucx://127.0.0.1:45196</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:9001/status' target='_blank'>http://127.0.0.1:9001/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>7</li>\n",
       "  <li><b>Cores: </b>7</li>\n",
       "  <li><b>Memory: </b>540.95 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'ucx://127.0.0.1:45196' processes=7 threads=7, memory=540.95 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_start = time.time()\n",
    "\n",
    "enable_tcp_over_ucx = True\n",
    "enable_nvlink = True\n",
    "enable_infiniband = True\n",
    "CUDA_VISIBLE_DEVICES = cuda_visible_devices(0).split(',')\n",
    "CUDA_VISIBLE_DEVICES = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# rmm.reinitialize(managed_memory=False)\n",
    "cp.cuda.set_allocator(rmm.rmm_cupy_allocator)\n",
    "\n",
    "initialize.initialize(create_cuda_context=True,\n",
    "                      enable_tcp_over_ucx=enable_tcp_over_ucx,\n",
    "                      enable_nvlink=enable_nvlink,\n",
    "                      enable_infiniband=enable_infiniband)\n",
    "\n",
    "cluster = LocalCUDACluster(protocol=\"ucx\",\n",
    "                           dashboard_address=':9001',\n",
    "                           CUDA_VISIBLE_DEVICES=CUDA_VISIBLE_DEVICES,\n",
    "                           enable_tcp_over_ucx=enable_tcp_over_ucx,\n",
    "                           enable_nvlink=enable_nvlink,\n",
    "                           enable_infiniband=enable_infiniband)\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "#TODO: Fix for freezes\n",
    "client.run(cp.cuda.set_allocator)\n",
    "\n",
    "n_workers = len(client.scheduler_info()['workers'].keys())\n",
    "print(n_workers)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we provide the path to the sparse `.h5ad` file containing the count matrix to analyze. Please see the README for instructions on how to download the dataset we use here.\n",
    "\n",
    "To run this notebook using your own dataset, please see the README for instructions to convert your own count matrix into this format. Then, replace the path in the cell below with the path to your generated `.h5ad` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, wget\n",
    "\n",
    "# input_file = \"../data/krasnow_hlca_10x.sparse.h5ad\"\n",
    "input_file = \"../data/1M_brain_cells_10X.sparse.h5ad\"\n",
    "if not os.path.exists(input_file):\n",
    "    print('Downloading import file...')\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    wget.download(\n",
    "        'https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',\n",
    "        input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marker genes\n",
    "MITO_GENE_PREFIX = \"mt-\"              # Prefix for mitochondrial genes to regress out\n",
    "markers = [\"Stmn2\", \"Hes1\", \"Olig1\"]  # Marker genes for visualization\n",
    "\n",
    "# filtering cells\n",
    "min_genes_per_cell = 200      # Filter out cells with fewer genes than this expressed \n",
    "max_genes_per_cell = 6000     # Filter out cells with more genes than this expressed \n",
    "min_cells=1                   # Genes containing a number of cells below this value will be filtered\n",
    "\n",
    "# filtering genesinitialize\n",
    "n_top_genes = 4000            # Number of highly variable genes to retain\n",
    "\n",
    "# PCA\n",
    "n_components = 50             # Number of principal components to compute\n",
    "pca_train_ratio = 0.35        # percentage of cells to use for PCA training\n",
    "n_pca_batches = 10\n",
    "\n",
    "# t-SNE\n",
    "tsne_n_pcs = 20               # Number of principal components to use for t-SNE\n",
    "\n",
    "# k-means\n",
    "k = 35                        # Number of clusters for k-means\n",
    "\n",
    "# KNN\n",
    "n_neighbors = 15              # Number of nearest neighbors for KNN graph\n",
    "knn_n_pcs = 50                # Number of principal components to use for finding nearest neighbors\n",
    "\n",
    "# UMAP\n",
    "umap_train_ratio = 0.35\n",
    "umap_min_dist = 0.3\n",
    "umap_spread = 1.0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "MAX_RECS = -1             # To filter first x numbers of rows. Please change it to -1 for using all\n",
    "BATCHSIZE = 40000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We load the sparse count matrix from an `h5ad` file using Scanpy. The sparse count matrix will then be placed on the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 319 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@delayed\n",
    "def read_partition(sample_file, ds_data, ds_indices, ds_indptr, \n",
    "                   batch_start, rows, total_cols, gene_filter,\n",
    "                   min_genes_per_cell=200, max_genes_per_cell=6000, target_sum=1e4):\n",
    "    \"\"\"\n",
    "    Loads a single partition from HDF5 file.\n",
    "    \"\"\"\n",
    "    batch_end = batch_start + rows\n",
    "\n",
    "    with h5py.File(input_file, 'r') as h5f:\n",
    "        # Read all things row pointers for one worker\n",
    "        indptrs = h5f[ds_indptr]\n",
    "        start_ptr = indptrs[batch_start]\n",
    "        end_ptr = indptrs[batch_end]\n",
    "        \n",
    "        # Read all things data for one worker\n",
    "        data = h5f[ds_data]\n",
    "        sub_data = cp.array(data[start_ptr:end_ptr])\n",
    "\n",
    "        # Read all things column pointers for one worker\n",
    "        indices = h5f[ds_indices]\n",
    "        sub_indices = cp.array(indices[start_ptr:end_ptr])\n",
    "\n",
    "        # recompute the row pointer for the partial dataset\n",
    "        sub_indptrs  = cp.array(indptrs[batch_start:(batch_end + 1)])\n",
    "        first_ptr = sub_indptrs[0]\n",
    "        sub_indptrs = sub_indptrs - first_ptr\n",
    "        \n",
    "    partial_sparse_array = cp.sparse.csr_matrix((sub_data, sub_indices, sub_indptrs),\n",
    "                                                shape=(batch_end - batch_start, total_cols))\n",
    "\n",
    "    # TODO: Add barcode filtering here.\n",
    "    degrees = cp.diff(partial_sparse_array.indptr)\n",
    "    query = ((min_genes_per_cell <= degrees) & (degrees <= max_genes_per_cell))\n",
    "    partial_sparse_array = partial_sparse_array[query]\n",
    "\n",
    "    if not gene_filter:\n",
    "        partial_sparse_sq_array = cp.sparse.csr_matrix((cp.square(sub_data), sub_indices, sub_indptrs),\n",
    "                                                shape=(batch_end - batch_start, total_cols))\n",
    "        partial_sparse_sq_array = partial_sparse_sq_array[query]\n",
    "        col1 = partial_sparse_array.sum(axis=0)\n",
    "        col2 = partial_sparse_sq_array.sum(axis=0)\n",
    "        ret_value = cp.hstack([col1, col2])\n",
    "        del partial_sparse_sq_array\n",
    "        \n",
    "    else:\n",
    "        partial_sparse_array = rapids_scanpy_funcs.normalize_total(\n",
    "            partial_sparse_array, target_sum=target_sum)\n",
    "        ret_value = partial_sparse_array[:, gene_filter]\n",
    "\n",
    "    del partial_sparse_array\n",
    "    return ret_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing sum and creating gene count filter...\n",
      "CPU times: user 8.08 s, sys: 1.3 s, total: 9.37 s\n",
      "Wall time: 47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24002,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read along with filtering\n",
    "with h5py.File(input_file, 'r') as h5f:\n",
    "    indptr = h5f['/X/indptr']\n",
    "    data = h5f['/X/data']\n",
    "    orginal_genes = h5f['/var/_index']\n",
    "    \n",
    "    orginal_genes = cudf.Series(orginal_genes, dtype=cp.dtype('object'))\n",
    "\n",
    "    total_cols = orginal_genes.shape[0]\n",
    "    total_rows = indptr.shape[0] - 1\n",
    "\n",
    "if MAX_RECS == -1:\n",
    "    MAX_RECS = total_rows\n",
    "\n",
    "print('Computing sum and creating gene count filter...')\n",
    "dls_sum = []\n",
    "for batch_start in range(0, MAX_RECS, BATCHSIZE):\n",
    "    batch_size = BATCHSIZE if MAX_RECS > batch_start + BATCHSIZE else MAX_RECS - batch_start\n",
    "    dls_sum.append(\n",
    "        dask.array.from_delayed(\n",
    "            (read_partition)(input_file, \n",
    "                           '/X/data', '/X/indices', '/X/indptr', \n",
    "                           batch_start, batch_size, total_cols, None,\n",
    "                           min_genes_per_cell=min_genes_per_cell,\n",
    "                           max_genes_per_cell=max_genes_per_cell),\n",
    "            dtype=cp.float32,\n",
    "            shape=(batch_size, total_cols * 2)))\n",
    "\n",
    "# First half of this array is sum and rest is square of sum\n",
    "sum_gpu_arrays  = dask.array.concatenate(dls_sum).compute()\n",
    "\n",
    "# Split the sum and square\n",
    "sum_gpu_array = sum_gpu_arrays[:, 0:sum_gpu_arrays.shape[1]/2]\n",
    "sum_sq_gpu_array = sum_gpu_arrays[:,sum_gpu_arrays.shape[1]/2:]\n",
    "\n",
    "# Filter genes with at-least <<min_cells>> number of cells rec.\n",
    "sum_gpu_array = sum_gpu_array.sum(axis=0)\n",
    "min_cell_filter = (sum_gpu_array >= min_cells)\n",
    "\n",
    "sum_gpu_array = sum_gpu_array[min_cell_filter]\n",
    "sum_sq_gpu_array = sum_sq_gpu_array[:, min_cell_filter]\n",
    "sum_sq_gpu_array = sum_sq_gpu_array.sum(axis=0)\n",
    "\n",
    "sum_sq_gpu_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into dataframes...\n",
      "Concate sub-arrays...\n",
      "Compute and persist arrays...\n",
      "CPU times: user 4.84 s, sys: 236 ms, total: 5.08 s\n",
      "Wall time: 5.02 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 125.40 GB </td> <td> 3.84 GB </td></tr>\n",
       "    <tr><th> Shape </th><td> (1306127, 24002) </td> <td> (40000, 24002) </td></tr>\n",
       "    <tr><th> Count </th><td> 33 Tasks </td><td> 33 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"78\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"28\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"3\" x2=\"28\" y2=\"3\" />\n",
       "  <line x1=\"0\" y1=\"11\" x2=\"28\" y2=\"11\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"28\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"22\" x2=\"28\" y2=\"22\" />\n",
       "  <line x1=\"0\" y1=\"29\" x2=\"28\" y2=\"29\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"28\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"28\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"47\" x2=\"28\" y2=\"47\" />\n",
       "  <line x1=\"0\" y1=\"55\" x2=\"28\" y2=\"55\" />\n",
       "  <line x1=\"0\" y1=\"62\" x2=\"28\" y2=\"62\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"28\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"73\" x2=\"28\" y2=\"73\" />\n",
       "  <line x1=\"0\" y1=\"80\" x2=\"28\" y2=\"80\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"28\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"95\" x2=\"28\" y2=\"95\" />\n",
       "  <line x1=\"0\" y1=\"99\" x2=\"28\" y2=\"99\" />\n",
       "  <line x1=\"0\" y1=\"106\" x2=\"28\" y2=\"106\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"28\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"28\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"28\" y1=\"0\" x2=\"28\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 28.558075100484363,0.0 28.558075100484363,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"14.279038\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >24002</text>\n",
       "  <text x=\"48.558075\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,48.558075,60.000000)\">1306127</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<concatenate, shape=(1306127, 24002), dtype=float32, chunksize=(40000, 24002), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Loading data into dataframes...')\n",
    "dls_data = []\n",
    "for batch_start in range(0, MAX_RECS, BATCHSIZE):\n",
    "    batch_size = BATCHSIZE if MAX_RECS > batch_start + BATCHSIZE else MAX_RECS - batch_start\n",
    "\n",
    "    dls_data.append(\n",
    "        dask.array.from_delayed(\n",
    "            (read_partition)(input_file,\n",
    "                           '/X/data', '/X/indices', '/X/indptr', \n",
    "                           batch_start, batch_size, total_cols, min_cell_filter.tolist(),\n",
    "                           min_genes_per_cell=min_genes_per_cell,\n",
    "                           max_genes_per_cell=max_genes_per_cell),\n",
    "            dtype=cp.float32,\n",
    "            shape=(batch_size, sum_sq_gpu_array.shape[0])))\n",
    "\n",
    "print('Concate sub-arrays...')\n",
    "sparse_gpu_array = dask.array.concatenate(dls_data)\n",
    "\n",
    "print('Compute and persist arrays...')\n",
    "type(sparse_gpu_array)\n",
    "sparse_gpu_array = sparse_gpu_array.persist()\n",
    "sparse_gpu_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we select the first 1 million cells in the dataset. We maintain the index of unique genes in our dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the shape of the resulting sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_gpu_array = dask.array.log1p(sparse_gpu_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the number of non-zero values in the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data load and format time: 52.11346769332886\n"
     ]
    }
   ],
   "source": [
    "print(\"Total data load and format time: %s\" % (time.time() - data_load_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24002,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out the genes that got removed from the original data\n",
    "genes = orginal_genes[min_cell_filter]\n",
    "genes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the count matrix to remove cells with an extreme number of genes expressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FILTERING IS NOT DONE WHILE READING DATA\n",
    "# sparse_gpu_array = rapids_scanpy_funcs.filter_cells(sparse_gpu_array, \n",
    "#                                                     min_genes=min_genes_per_cell, \n",
    "#                                                     max_genes=max_genes_per_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some genes will now have zero expression in all cells. We filter out such genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FILTERING IS NOT DONE WHILE READING DATA\n",
    "# sparse_gpu_array, genes = rapids_scanpy_funcs.filter_genes(sparse_gpu_array, genes.get(), min_cells=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of our count matrix is now reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sparse_gpu_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the count matrix so that the total counts in each cell sum to 1e4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This is now done while loading data\n",
    "# sparse_gpu_array = rapids_scanpy_funcs.normalize_total(sparse_gpu_array, target_sum=1e4)\n",
    "# sparse_gpu_array = sparse_gpu_array.log1p()\n",
    "\n",
    "# sparse_gpu_array = sparse_gpu_array.astype(cp.float32)\n",
    "# sparse_gpu_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we log transform the count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.6 s, sys: 564 ms, total: 3.16 s\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "marker_genes_raw = {}\n",
    "genes1 = genes.reset_index(drop=True)\n",
    "i = 0\n",
    "for index in genes1[genes1.isin(markers)].index.to_arrow().to_pylist():\n",
    "    marker_genes_raw[markers[i]] = sparse_gpu_array[:, index].compute().toarray().ravel()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Most Variable Genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the count matrix to an annData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gpu_array = sparse_gpu_array.compute()\n",
    "# adata = anndata.AnnData(gpu_array.get())\n",
    "# adata.var_names = genes.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before filtering the count matrix, we save the 'raw' expression values of the marker genes to use for labeling cells afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scanpy, we filter the count matrix to retain only the most variable genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor=\"cell_ranger\")\n",
    "# print(\"Full time: %s\" % (time.time() - start))\n",
    "# sc_hightly_variable = genes[adata.var.highly_variable.values]\n",
    "# sc_hightly_variable.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 56 ms, total: 352 ms\n",
      "Wall time: 342 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mean = sum_gpu_array / total_rows\n",
    "mean[mean == 0] = 1e-12\n",
    "\n",
    "mean_sq = sum_sq_gpu_array / total_rows\n",
    "variance = mean_sq - mean ** 2\n",
    "variance *= total_cols / (total_rows - 1) \n",
    "dispersion = variance / mean\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['means'] = mean.tolist()\n",
    "df['dispersions'] = dispersion.tolist()\n",
    "df['mean_bin'] = pd.cut(\n",
    "    df['means'],\n",
    "    np.r_[-np.inf, np.percentile(df['means'], np.arange(10, 105, 5)), np.inf],\n",
    ")\n",
    "\n",
    "\n",
    "disp_grouped = df.groupby('mean_bin')['dispersions']\n",
    "disp_median_bin = disp_grouped.median()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    disp_mad_bin = disp_grouped.apply(robust.mad)\n",
    "    df['dispersions_norm'] = (\n",
    "        df['dispersions'].values - disp_median_bin[df['mean_bin'].values].values\n",
    "    ) / disp_mad_bin[df['mean_bin'].values].values\n",
    "\n",
    "dispersion_norm = df['dispersions_norm'].values\n",
    "dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]\n",
    "dispersion_norm[::-1].sort()\n",
    "\n",
    "if n_top_genes > df.shape[0]:\n",
    "    n_top_genes = df.shape[0]\n",
    "\n",
    "disp_cut_off = dispersion_norm[n_top_genes - 1]\n",
    "vaiable_genes = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off\n",
    "genes = genes[vaiable_genes]\n",
    "genes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regress out confounding factors (number of counts, mitochondrial gene expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform regression on the count matrix to correct for confounding factors -  for example purposes, we use the number of counts and the expression of mitochondrial genes (named starting with `mt-`).\n",
    "\n",
    "We now calculate the total counts and the percentage of mitochondrial counts for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306127, 4000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = int(sparse_gpu_array.shape[0] / (n_workers*10))\n",
    "\n",
    "# Filter highly variable genes\n",
    "dask_array = sparse_gpu_array[:, vaiable_genes]\n",
    "\n",
    "sparse_gpu_array.shape\n",
    "dask_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 6.17 s, total: 18.3 s\n",
      "Wall time: 21.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1291337, 4000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@with_cupy_rmm\n",
    "def csr_to_csc(csr_array, client):\n",
    "    def _conv_csr_to_csc(x):\n",
    "        return  x.tocsc()\n",
    "\n",
    "    parts = client.sync(_extract_partitions, csr_array)\n",
    "    futures = [client.submit(_conv_csr_to_csc, \n",
    "                             part, \n",
    "                             workers=[w], \n",
    "                             pure=False)\n",
    "               for w, part in parts]\n",
    "    objs = []\n",
    "    shape = csr_array.shape\n",
    "    for i in range(len(futures)):\n",
    "        obj = dask.array.from_delayed(futures[i], \n",
    "                                      shape=futures[i].result().shape,\n",
    "                                      dtype=cp.float32)\n",
    "        objs.append(obj)\n",
    "    return dask.array.concatenate(objs)\n",
    "\n",
    "sparse_gpu_array = csr_to_csc(dask_array, client=client)\n",
    "sparse_gpu_array = sparse_gpu_array.persist()\n",
    "sparse_gpu_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 184 ms, total: 1.24 s\n",
      "Wall time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@with_cupy_rmm\n",
    "def sum_csc(csc_array, client):\n",
    "\n",
    "    shape = csc_array.shape\n",
    "    def __sum(x):\n",
    "        return x.sum(axis=1)\n",
    "\n",
    "    parts = client.sync(_extract_partitions, csc_array)\n",
    "    futures = [client.submit(__sum, \n",
    "                             part, \n",
    "                             workers=[w], \n",
    "                             pure=False)\n",
    "               for w, part in parts]\n",
    "    objs = []\n",
    "    for i in range(len(futures)):\n",
    "        obj = dask.array.from_delayed(futures[i], \n",
    "                                      shape=futures[i].result().shape,\n",
    "                                      dtype=cp.float32)\n",
    "        objs.append(obj)\n",
    "    return dask.array.concatenate(objs)\n",
    "\n",
    "mito_genes = genes.str.startswith(MITO_GENE_PREFIX)\n",
    "mito_genes = cp.fromDlpack(mito_genes.to_dlpack())\n",
    "mito_genes_indices = cp.where(mito_genes == 1)[0]\n",
    "\n",
    "# # n_counts = dask.array.sum(sparse_gpu_array, axis=1, keepdims=False)\n",
    "n_counts = sum_csc(sparse_gpu_array, client=client)\n",
    "n_counts = dask.array.sum(n_counts, axis=1).compute()\n",
    "\n",
    "result = dask.array.take(sparse_gpu_array,\n",
    "                         mito_genes_indices.tolist(),\n",
    "                         axis=1)\n",
    "result = sum_csc(result, client=client)\n",
    "result = dask.array.sum(result, axis=1)\n",
    "result = dask.array.true_divide(result, n_counts)\n",
    "# result.compute()\n",
    "\n",
    "percent_mito = dask.array.ravel(result)\n",
    "percent_mito = percent_mito.compute()\n",
    "del result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 11 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1291337, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# dk_sparse_gpu_array = dask.array.from_array(sparse_gpu_array, chunks=chunk_size, asarray=False)\n",
    "regressors = cp.ones((n_counts.shape[0]*3), dtype=cp.float32).reshape((n_counts.shape[0], 3), order=\"F\")\n",
    "regressors[:, 1] = n_counts\n",
    "regressors[:, 2] = percent_mito\n",
    "\n",
    "X = dask.array.from_array(regressors, asarray=False)\n",
    "del regressors\n",
    "del n_counts\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def regress(X, arr, client, max_value=10):\n",
    "#     shape = arr.shape\n",
    "\n",
    "#     def __regress(X, y):\n",
    "#         y = y.compute()\n",
    "#         print(y.shape)\n",
    "#         if hasattr(y, 'todense'):\n",
    "#             y = y.todense()\n",
    "\n",
    "#         outputs = cp.empty(y.shape, dtype=y.dtype, order=\"F\")\n",
    "#         for i in range(y.shape[1]):\n",
    "#             lr = LinearRegression(fit_intercept=False, output_type=\"cupy\")\n",
    "#             lr.fit(X, y[i], convert_dtype=True)\n",
    "# #             outputs[:, i] = y.reshape(y.shape[0], ) - lr.predict(X).reshape(y.shape[0])\n",
    "\n",
    "#         return outputs\n",
    "\n",
    "#     print('Submitting regression jobs and obtain future objects...', arr[:, 0:10])\n",
    "#     futures = [client.submit(__regress, X, arr[:, i:i+10], pure=False)\n",
    "#                for i in range(0, arr.shape[1], 10)]\n",
    "\n",
    "#     print('Collecting results from futures...', len(futures[0].result().shape))\n",
    "#     objs = []\n",
    "#     for i in range(len(futures)):\n",
    "#         obj = dask.array.from_delayed(futures[i],\n",
    "#                                       shape=futures[i].result().shape,\n",
    "#                                       dtype=cp.float32)\n",
    "#         objs.append(obj)\n",
    "\n",
    "#     print('Stack and return the result...')\n",
    "#     return dask.array.vstack(objs)\n",
    "\n",
    "# result = regress(regressors, sparse_gpu_array, client)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dask tasks for LinearRegression...\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "Computing...\n",
      "CPU times: user 1.66 s, sys: 180 ms, total: 1.84 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# sparse_gpu_array = rapids_scanpy_funcs.regress_out(\n",
    "#     sparse_gpu_array, n_counts, percent_mito)\n",
    "# sparse_gpu_array.shape\n",
    "\n",
    "def regress(regressors, arr, cols_per_task=10):\n",
    "    \n",
    "    @delayed\n",
    "    def _regress(X, data):\n",
    "        if hasattr(data, 'todense'):\n",
    "            data = data.todense()\n",
    "\n",
    "        outputs = cp.empty(data.shape, dtype=data.dtype, order=\"F\")\n",
    "\n",
    "        for i in range(data.shape[1]):\n",
    "            y = data[:, i]\n",
    "            lr = LinearRegression(fit_intercept=False, output_type=\"cupy\")\n",
    "            lr.fit(X, y, convert_dtype=True)\n",
    "            outputs[:, i] = y.reshape(y.shape[0], ) - lr.predict(X).reshape(y.shape[0])\n",
    "        return outputs\n",
    "\n",
    "    print('Creating dask tasks for LinearRegression...')\n",
    "    ld_delay = []\n",
    "    row_cnt = arr.shape[0]\n",
    "    for i in range(0, arr.shape[1], cols_per_task):\n",
    "        if i % 500 == 0: print(i)\n",
    "\n",
    "        y = arr[:, i:i+2]\n",
    "        ld_delay.append(\n",
    "            dask.array.from_delayed(\n",
    "                (_regress)(regressors, y),\n",
    "                dtype=cp.float32,\n",
    "                shape=(arr.shape[0], cols_per_task)))\n",
    "\n",
    "    print('Computing...')\n",
    "    result = dask.array.concatenate(ld_delay, axis=1)\n",
    "#     result = dask.compute(ld_delay)\n",
    "    # result = dask.array.vstack(result[0]).T\n",
    "    return result\n",
    "\n",
    "# TODO: Creating this bottleneck because it is twice as faster\n",
    "result = regress(X, sparse_gpu_array, cols_per_task=10)\n",
    "# result = result.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 2.17 s, total: 20.7 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "type(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9907, 4000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we scale the count matrix to obtain a z-score and apply a cutoff value of 10 standard deviations, obtaining the preprocessed count matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1610067994.6956713\n",
      "2 1610067994.6966562\n",
      "Before sync\n",
      "creating futures\n",
      "created  2000  futures\n",
      "1610068170.1046958 0\n",
      "1610068170.1047254 (1291337, 2)\n",
      "1610068170.1683943 <Future: finished, type: cupy.ndarray, key: __clip-a8f7d73e-031e-4ef1-a22c-c6482f5b66ee>\n",
      "1610068170.1684358 0\n",
      "1610068218.9440322 500\n",
      "1610068218.9448462 (1291337, 2)\n",
      "1610068219.0244772 <Future: finished, type: cupy.ndarray, key: __clip-88eb6a70-8879-414b-aaac-c0f84a128721>\n",
      "1610068219.0245245 500\n",
      "1610068266.5767832 1000\n",
      "1610068266.5771763 (1291337, 2)\n",
      "1610068266.652241 <Future: finished, type: cupy.ndarray, key: __clip-f7fe0f8b-3eb3-4a10-8870-54ea662d730f>\n",
      "1610068266.6525278 1000\n",
      "1610068312.5286858 1500\n",
      "1610068312.5288424 (1291337, 2)\n",
      "1610068312.5801935 <Future: finished, type: cupy.ndarray, key: __clip-a2b369f3-ce61-4aaa-bacc-0bd1511f5521>\n",
      "1610068312.580247 1500\n",
      "collected  2000  results\n",
      "Part 3 1610068359.086228\n",
      "CPU times: user 3min 24s, sys: 56 s, total: 4min 20s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sparse_gpu_array = rapids_scanpy_funcs.scale(sparse_gpu_array, max_value=10)\n",
    "\n",
    "max_value=10\n",
    "\n",
    "\n",
    "@with_cupy_rmm\n",
    "def clip(arr, client, max_value=10):\n",
    "\n",
    "    shape = arr.shape\n",
    "    def __clip(x, max_value):\n",
    "        return x.clip(a_max=max_value)\n",
    "\n",
    "    print('Before sync')\n",
    "    parts = client.sync(_extract_partitions, arr)\n",
    "    print('creating futures')\n",
    "    futures = [client.submit(__clip, \n",
    "                             part,\n",
    "                             max_value,\n",
    "                             workers=[w], \n",
    "                             pure=False)\n",
    "               for w, part in parts]\n",
    "    objs = []\n",
    "    print('created ', len(futures), ' futures')\n",
    "    for i in range(len(futures)):\n",
    "        if i % 500 == 0:\n",
    "        obj = dask.array.from_delayed(futures[i],\n",
    "                                      shape=futures[i].result().shape,\n",
    "                                      dtype=cp.float32)\n",
    "        objs.append(obj)\n",
    "    print('collected ', len(objs), ' results')\n",
    "    return dask.array.concatenate(objs, axis=1)\n",
    "\n",
    "sparse_gpu_array = result\n",
    "mean = dask.array.mean(sparse_gpu_array, axis=0)\n",
    "temp_array = dask.array.subtract(sparse_gpu_array, mean)\n",
    "\n",
    "stddev = dask.array.sqrt(mean.var())\n",
    "temp_array = dask.array.true_divide(temp_array, stddev)\n",
    "client.rebalance()\n",
    "print('1', time.time())\n",
    "# sparse_gpu_array\n",
    "print('2', time.time())\n",
    "result = clip(temp_array, client, max_value)\n",
    "print('Part 3', time.time())\n",
    "\n",
    "del stddev\n",
    "del temp_array\n",
    "del mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster & Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the preprocessed count matrix as an AnnData object, which is currently in host memory. We also add the expression levels of the marker genes as observations to the annData object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PCA to reduce the dimensionality of the matrix to its top 50 principal components. Here, we use Dask to parallelize across multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA multi-GPU time includes initial data transfer, which is about 20gb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending delayed function returning cudf dataframe...\n",
      "Creating dask df from delays...\n",
      "Computing delayed functions...\n",
      "Creating Dataframe from futures...\n",
      "CPU times: user 51.1 s, sys: 3.61 s, total: 54.7 s\n",
      "Wall time: 53.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process still alive after 3 seconds, killing\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# dask_df = dask.dataframe.from_array(sparse_gpu_array, columns=gene.to_arrow().to_pylist())\n",
    "# dask_cu_df = dask_cudf.from_dask_dataframe(dask_df)\n",
    "#\n",
    "# The above line does not work due to worker failure while converting sparse\n",
    "# array to dataframes. Folloiwng code is a workaround for loading data using \n",
    "# delayed functions.\n",
    "\n",
    "@delayed\n",
    "def read_sparse_gpu_array(sparse_array, start, batch_size):\n",
    "    return cudf.DataFrame(sparse_array[start: start + batch_size])\n",
    "\n",
    "BATCHES = n_workers * 10\n",
    "num_recs = sparse_gpu_array.shape[0]\n",
    "batch_size = math.ceil(num_recs / BATCHES)\n",
    "columns = genes.to_arrow().to_pylist()\n",
    "\n",
    "print(\"Appending delayed function returning cudf dataframe...\")\n",
    "dls = []\n",
    "for start in range(0, num_recs, batch_size):\n",
    "    bsize = min(num_recs - start, batch_size)\n",
    "    dls.append(read_sparse_gpu_array(sparse_gpu_array, start, bsize))\n",
    "\n",
    "import pandas\n",
    "\n",
    "print(\"Creating dask df from delays...\")\n",
    "prop_meta = {i: pandas.Series([], dtype='float32') for i in range(sparse_gpu_array.shape[1])}\n",
    "meta_df = cudf.DataFrame(prop_meta)\n",
    "\n",
    "print(\"Computing delayed functions...\")\n",
    "dls = client.compute(dls)\n",
    "\n",
    "print(\"Creating Dataframe from futures...\")\n",
    "dask_cu_df = dask.dataframe.from_delayed(dls, meta=meta_df)\n",
    "dask_cu_df = dask_cu_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/cuml/dask/common/part_utils.py\u001b[0m in \u001b[0;36m_extract_partitions\u001b[0;34m(dask_obj, client)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     raise gen.Return([(first(who_has[key]), part)\n\u001b[0m\u001b[1;32m    160\u001b[0m                       for key, part in key_to_part])\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/cuml/dask/common/part_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     raise gen.Return([(first(who_has[key]), part)\n\u001b[0m\u001b[1;32m    160\u001b[0m                       for key, part in key_to_part])\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/toolz/itertoolz.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/cuml/dask/decomposition/pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \"\"\"\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/cuml/dask/decomposition/base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, _transform)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mn_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistributedDataHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/cuml/dask/common/input_utils.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, data, client)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_datatype_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mgpu_futures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_partitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_futures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m             return sync(\n\u001b[0m\u001b[1;32m    833\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m                             \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                         \u001b[0myielded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReturn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = cu_dask_PCA(n_components=n_components, client=client)\n",
    "# dask_reduced_df = pca.fit_transform(dask_cu_df)\n",
    "\n",
    "pca.fit(dask_cu_df)\n",
    "dask_reduced_df = pca.transform(dask_cu_df)\n",
    "\n",
    "# TODO: Following line is a bottleneck\n",
    "reduced_df = dask_reduced_df.compute()\n",
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE + k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: Bottleneck\n",
    "tsne = TSNE().fit_transform(reduced_df.iloc[:, :tsne_n_pcs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cluster the cells using k-means on the principal components. For example purposes, we set k=35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# K-means\n",
    "dask_array = dask_cudf.from_cudf(reduced_df, npartitions=BATCHES)\n",
    "dask_kmeans_output = cu_dask_KMeans(n_clusters=k).fit_predict(dask_array)\n",
    "kmeans = dask_kmeans_output.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.index = kmeans.index\n",
    "tsne['kmeans'] = kmeans\n",
    "for marker_gene in marker_genes_raw:\n",
    "    print(marker_genes_raw[marker_gene].shape)\n",
    "    tsne[marker_gene + '_raw'] = marker_genes_raw[marker_gene]\n",
    "tsne.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the cells using t-SNE and label cells by color according to the k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def show_tsne(df, x, y, cluster_col, title):\n",
    "    tsne_fig = figure(title=title, width=800, output_backend=\"webgl\")\n",
    "    clusters = df[cluster_col].unique().values_host\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cdf = df.query(cluster_col + ' == ' + str(cluster))\n",
    "        if cdf.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        x_array = cp.fromDlpack(cdf[0].to_dlpack())\n",
    "        y_array = cp.fromDlpack(cdf[1].to_dlpack())\n",
    "\n",
    "        tsne_fig.circle(x_array.get(),\n",
    "                        y_array.get(),\n",
    "                        size=2,\n",
    "                        color=COLORS[cluster],\n",
    "                        legend = 'Cluster ' + str(cluster))\n",
    "\n",
    "    tsne_fig.legend.location = 'top_right'\n",
    "    tsne_fig.legend.title = 'Clusters'\n",
    "\n",
    "    tsne_fig_handle = show(tsne_fig, notebook_handle=True)\n",
    "    push_notebook(handle=tsne_fig_handle)\n",
    "\n",
    "show_tsne(tsne, 0, 1, 'kmeans', 'kmeans')\n",
    "\n",
    "# sc.pl.tsne(adata, color=[\"kmeans\"])\n",
    "# x_embedded = TSNE().fit_transform(sparse_gpu_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label the cells using the `Stmn2` and `Hes1` marker genes, for neuronal and glial cells respectively. These visualizations show us the separation of neuronal and glial cells on the t-SNE plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def show_tsne_grad(df, x, y, cluster_col, color_col, title):\n",
    "    tsne_fig = figure(title=title, width=800, output_backend=\"webgl\")\n",
    "    clusters = df[cluster_col].unique().values_host\n",
    "    for cluster in clusters:\n",
    "        cdf = df.query(cluster_col + ' == ' + str(cluster))\n",
    "        if cdf.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        x_array = cp.fromDlpack(cdf[0].to_dlpack())\n",
    "        y_array = cp.fromDlpack(cdf[1].to_dlpack())\n",
    "        color_array = cp.fromDlpack(cdf[color_col].to_dlpack())\n",
    "        tsne_fig.circle(x_array.get(),\n",
    "                        y_array.get(),\n",
    "                        size=2,\n",
    "                        color=color_array.get(),\n",
    "                        legend = 'Cluster ' + str(cluster))\n",
    "\n",
    "    tsne_fig.legend.location = 'top_right'\n",
    "    tsne_fig.legend.title = 'Clusters'\n",
    "\n",
    "    tsne_fig_handle = show(tsne_fig, notebook_handle=True)\n",
    "    push_notebook(handle=tsne_fig_handle)\n",
    "\n",
    "show_tsne_grad(tsne, 0, 1, 'kmeans', 'Stmn2_raw', 'Stmn2')\n",
    "show_tsne_grad(tsne, 0, 1, 'kmeans', 'Hes1_raw', 'Stmn2')\n",
    "\n",
    "# sc.pl.tsne(adata, color=[\"Stmn2_raw\"], color_map=\"Blues\", vmax=1, vmin=-0.05)\n",
    "# sc.pl.tsne(adata, color=[\"Hes1_raw\"], color_map=\"Blues\", vmax=1, vmin=-0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP + Graph clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the cells using the UMAP algorithm in Rapids. Before UMAP, we need to construct a k-nearest neighbors graph in which each cell is connected to its nearest neighbors. This can be done conveniently using rapids functionality already integrated into Scanpy.\n",
    "\n",
    "Note that Scanpy uses an approximation to the nearest neighbors on the CPU while the GPU version performs an exact search. While both methods are known to yield useful results, some differences in the resulting visualization and clusters can be observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UMAP function from Rapids is also integrated into Scanpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_model = UMAP(n_epochs=1000, min_dist=umap_min_dist, spread=umap_spread)\n",
    "local_model.fit(reduced_df[:350000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dist_embeddings = cu_dask_UMAP(local_model).transform(dask_array)\n",
    "X_umap = dist_embeddings.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the Louvain algorithm for graph-based clustering, once again using the `rapids` option in Scanpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import cugraph as cg\n",
    "import cudf\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=n_neighbors, output_type=\"input\")\n",
    "model.fit(sparse_gpu_array)\n",
    "\n",
    "graph = model.kneighbors_graph(X=sparse_gpu_array, n_neighbors=n_neighbors, mode='connectivity') \n",
    "\n",
    "# distances, indices = model.kneighbors(reduced_df)\n",
    "# nn = cu_dask_NearestNeighbors(client=client, n_neighbors=n_neighbors)\n",
    "# model = nn.fit(dask_reduced_df)\n",
    "# neighbors = nn.kneighbors(dask_reduced_df)\n",
    "# neighbors\n",
    "\n",
    "graph = graph.tocoo()\n",
    "edge_list = cudf.DataFrame({'g_row':graph.row, 'g_col':graph.col})\n",
    "G = cg.Graph()\n",
    "G.from_cudf_edgelist(edge_list, 'g_row', 'g_col')\n",
    "louvain_parts, score = cg.louvain(G)\n",
    "\n",
    "# sc.pp.neighbors(adata, n_neighbors=n_neighbors, n_pcs=knn_n_pcs, method='rapids')\n",
    "# sc.tl.louvain(adata, flavor='rapids')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the cells using the UMAP visualization, and using the Louvain clusters as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pl.umap(adata, color=[\"louvain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also visualize the cells labeled by expression of the `Stmn2` and `Hes1` marker genes, for neuronal and glial cells respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pl.umap(adata, color=[\"Stmn2_raw\"], color_map=\"Blues\", vmax=1, vmin=-0.05)\n",
    "sc.pl.umap(adata, color=[\"Hes1_raw\"], color_map=\"Blues\", vmax=1, vmin=-0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_time = time.time()\n",
    "print(\"Total cluster time : %s\" % (cluster_time-cluster_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Zoomed View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speedup offered by Rapids makes it easy to interactively re-analyze subsets of cells. To illustrate this, we select glial cells (Hes1+) from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hes1_cells = marker_genes_raw[\"Hes1_raw\"] > 0.0\n",
    "adata = adata[hes1_cells.get(),:]\n",
    "adata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the dimension reduction, clustering and visualization using this subset of cells in seconds. \n",
    "\n",
    "Finally, we visualize the selected neuronal cells labeled by their new clusters, and by the expression of `Olig1`, a marker gene for oligodendrocytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dask_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chunk_size = int(sparse_gpu_array.shape[0] / (n_workers*10))\n",
    "dask_array = dask.array.from_array(cp.asarray(adata.X), \n",
    "                                   chunks=(chunk_size, -1))\n",
    "dask_array = dask_array.persist()\n",
    "\n",
    "client.rebalance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask_array = pca.fit_transform(dask_array)\n",
    "adata.obsm['X_pca'] = dask_array.compute().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pp.neighbors(adata, n_neighbors=n_neighbors, n_pcs=knn_n_pcs, method='rapids')\n",
    "sc.tl.umap(adata, min_dist=umap_min_dist, spread=umap_spread, method='rapids')\n",
    "sc.tl.louvain(adata, flavor='rapids')\n",
    "\n",
    "sc.pl.umap(adata, color=[\"louvain\"])\n",
    "sc.pl.umap(adata, color=[\"Olig1_raw\"], color_map=\"Blues\", vmax=1, vmin=-0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reanalysis_time = time.time()\n",
    "print(\"Total reanalysis time : %s\" % (reanalysis_time-reanalysis_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full time: %s\" % (time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
